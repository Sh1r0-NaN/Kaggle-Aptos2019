{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from joblib import load, dump\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.vision.transform import *\n",
    "from fastai.callbacks import *\n",
    "from torchvision import models as md\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import re\n",
    "import math\n",
    "import collections\n",
    "from functools import partial\n",
    "from torch.utils import model_zoo\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import json\n",
    "import os, sys, gc\n",
    "import random\n",
    "import torch\n",
    "import ignite\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I could not figure out how to install package in local kernel so i just stole from github =)\n",
    "#code stolen from https://github.com/lukemelas/EfficientNet-PyTorch\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This file contains helper functions for building the model and for loading model parameters.\n",
    "These helper functions are built to mirror those in the official TensorFlow implementation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Parameters for the entire model (stem, all blocks, and head)\n",
    "GlobalParams = collections.namedtuple('GlobalParams', [\n",
    "    'batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate',\n",
    "    'num_classes', 'width_coefficient', 'depth_coefficient',\n",
    "    'depth_divisor', 'min_depth', 'drop_connect_rate', 'image_size'])\n",
    "\n",
    "\n",
    "# Parameters for an individual model block\n",
    "BlockArgs = collections.namedtuple('BlockArgs', [\n",
    "    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n",
    "    'expand_ratio', 'id_skip', 'stride', 'se_ratio'])\n",
    "\n",
    "\n",
    "# Change namedtuple defaults\n",
    "GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n",
    "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n",
    "\n",
    "\n",
    "def relu_fn(x):\n",
    "    \"\"\" Swish activation function \"\"\"\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def round_filters(filters, global_params):\n",
    "    \"\"\" Calculate and round number of filters based on depth multiplier. \"\"\"\n",
    "    multiplier = global_params.width_coefficient\n",
    "    if not multiplier:\n",
    "        return filters\n",
    "    divisor = global_params.depth_divisor\n",
    "    min_depth = global_params.min_depth\n",
    "    filters *= multiplier\n",
    "    min_depth = min_depth or divisor\n",
    "    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n",
    "    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n",
    "        new_filters += divisor\n",
    "    return int(new_filters)\n",
    "\n",
    "\n",
    "def round_repeats(repeats, global_params):\n",
    "    \"\"\" Round number of filters based on depth multiplier. \"\"\"\n",
    "    multiplier = global_params.depth_coefficient\n",
    "    if not multiplier:\n",
    "        return repeats\n",
    "    return int(math.ceil(multiplier * repeats))\n",
    "\n",
    "\n",
    "def drop_connect(inputs, p, training):\n",
    "    \"\"\" Drop connect. \"\"\"\n",
    "    if not training: return inputs\n",
    "    batch_size = inputs.shape[0]\n",
    "    keep_prob = 1 - p\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n",
    "    binary_tensor = torch.floor(random_tensor)\n",
    "    output = inputs / keep_prob * binary_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_same_padding_conv2d(image_size=None):\n",
    "    \"\"\" Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n",
    "        Static padding is necessary for ONNX exporting of models. \"\"\"\n",
    "    if image_size is None:\n",
    "        return Conv2dDynamicSamePadding\n",
    "    else:\n",
    "        return partial(Conv2dStaticSamePadding, image_size=image_size)\n",
    "\n",
    "class Conv2dDynamicSamePadding(nn.Conv2d):\n",
    "    \"\"\" 2D Convolutions like TensorFlow, for a dynamic image size \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
    "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]]*2\n",
    "\n",
    "    def forward(self, x):\n",
    "        ih, iw = x.size()[-2:]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2])\n",
    "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "class Conv2dStaticSamePadding(nn.Conv2d):\n",
    "    \"\"\" 2D Convolutions like TensorFlow, for a fixed image size\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, image_size=None, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n",
    "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
    "\n",
    "        # Calculate padding based on image size and save it\n",
    "        assert image_size is not None\n",
    "        ih, iw = image_size if type(image_size) == list else [image_size, image_size]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n",
    "        else:\n",
    "            self.static_padding = Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.static_padding(x)\n",
    "        x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input\n",
    "\n",
    "\n",
    "########################################################################\n",
    "############## HELPERS FUNCTIONS FOR LOADING MODEL PARAMS ##############\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def efficientnet_params(model_name):\n",
    "    \"\"\" Map EfficientNet model name to parameter coefficients. \"\"\"\n",
    "    params_dict = {\n",
    "        # Coefficients:   width,depth,res,dropout\n",
    "        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
    "        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
    "        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
    "        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
    "        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
    "        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
    "        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
    "        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
    "    }\n",
    "    return params_dict[model_name]\n",
    "\n",
    "\n",
    "class BlockDecoder(object):\n",
    "    \"\"\" Block Decoder for readability, straight from the official TensorFlow repository \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _decode_block_string(block_string):\n",
    "        \"\"\" Gets a block through a string notation of arguments. \"\"\"\n",
    "        assert isinstance(block_string, str)\n",
    "\n",
    "        ops = block_string.split('_')\n",
    "        options = {}\n",
    "        for op in ops:\n",
    "            splits = re.split(r'(\\d.*)', op)\n",
    "            if len(splits) >= 2:\n",
    "                key, value = splits[:2]\n",
    "                options[key] = value\n",
    "\n",
    "        # Check stride\n",
    "        assert (('s' in options and len(options['s']) == 1) or\n",
    "                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n",
    "\n",
    "        return BlockArgs(\n",
    "            kernel_size=int(options['k']),\n",
    "            num_repeat=int(options['r']),\n",
    "            input_filters=int(options['i']),\n",
    "            output_filters=int(options['o']),\n",
    "            expand_ratio=int(options['e']),\n",
    "            id_skip=('noskip' not in block_string),\n",
    "            se_ratio=float(options['se']) if 'se' in options else None,\n",
    "            stride=[int(options['s'][0])])\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_block_string(block):\n",
    "        \"\"\"Encodes a block to a string.\"\"\"\n",
    "        args = [\n",
    "            'r%d' % block.num_repeat,\n",
    "            'k%d' % block.kernel_size,\n",
    "            's%d%d' % (block.strides[0], block.strides[1]),\n",
    "            'e%s' % block.expand_ratio,\n",
    "            'i%d' % block.input_filters,\n",
    "            'o%d' % block.output_filters\n",
    "        ]\n",
    "        if 0 < block.se_ratio <= 1:\n",
    "            args.append('se%s' % block.se_ratio)\n",
    "        if block.id_skip is False:\n",
    "            args.append('noskip')\n",
    "        return '_'.join(args)\n",
    "\n",
    "    @staticmethod\n",
    "    def decode(string_list):\n",
    "        \"\"\"\n",
    "        Decodes a list of string notations to specify blocks inside the network.\n",
    "\n",
    "        :param string_list: a list of strings, each string is a notation of block\n",
    "        :return: a list of BlockArgs namedtuples of block args\n",
    "        \"\"\"\n",
    "        assert isinstance(string_list, list)\n",
    "        blocks_args = []\n",
    "        for block_string in string_list:\n",
    "            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n",
    "        return blocks_args\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(blocks_args):\n",
    "        \"\"\"\n",
    "        Encodes a list of BlockArgs to a list of strings.\n",
    "\n",
    "        :param blocks_args: a list of BlockArgs namedtuples of block args\n",
    "        :return: a list of strings, each string is a notation of block\n",
    "        \"\"\"\n",
    "        block_strings = []\n",
    "        for block in blocks_args:\n",
    "            block_strings.append(BlockDecoder._encode_block_string(block))\n",
    "        return block_strings\n",
    "\n",
    "\n",
    "def efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2,\n",
    "                 drop_connect_rate=0.2, image_size=None, num_classes=1000):\n",
    "    \"\"\" Creates a efficientnet model. \"\"\"\n",
    "\n",
    "    blocks_args = [\n",
    "        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n",
    "        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n",
    "        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n",
    "        'r1_k3_s11_e6_i192_o320_se0.25',\n",
    "    ]\n",
    "    blocks_args = BlockDecoder.decode(blocks_args)\n",
    "\n",
    "    global_params = GlobalParams(\n",
    "        batch_norm_momentum=0.99,\n",
    "        batch_norm_epsilon=1e-3,\n",
    "        dropout_rate=dropout_rate,\n",
    "        drop_connect_rate=drop_connect_rate,\n",
    "        # data_format='channels_last',  # removed, this is always true in PyTorch\n",
    "        num_classes=num_classes,\n",
    "        width_coefficient=width_coefficient,\n",
    "        depth_coefficient=depth_coefficient,\n",
    "        depth_divisor=8,\n",
    "        min_depth=None,\n",
    "        image_size=image_size,\n",
    "    )\n",
    "\n",
    "    return blocks_args, global_params\n",
    "\n",
    "\n",
    "def get_model_params(model_name, override_params):\n",
    "    \"\"\" Get the block args and global params for a given model \"\"\"\n",
    "    if model_name.startswith('efficientnet'):\n",
    "        w, d, s, p = efficientnet_params(model_name)\n",
    "        # note: all models have drop connect rate = 0.2\n",
    "        blocks_args, global_params = efficientnet(\n",
    "            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n",
    "    else:\n",
    "        raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n",
    "    if override_params:\n",
    "        # ValueError will be raised here if override_params has fields not included in global_params.\n",
    "        global_params = global_params._replace(**override_params)\n",
    "    return blocks_args, global_params\n",
    "\n",
    "\n",
    "url_map = {\n",
    "    'efficientnet-b0': 'http://storage.googleapis.com/public-models/efficientnet-b0-08094119.pth',\n",
    "    'efficientnet-b1': 'http://storage.googleapis.com/public-models/efficientnet-b1-dbc7070a.pth',\n",
    "    'efficientnet-b2': 'http://storage.googleapis.com/public-models/efficientnet-b2-27687264.pth',\n",
    "    'efficientnet-b3': 'http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth',\n",
    "    'efficientnet-b4': 'http://storage.googleapis.com/public-models/efficientnet-b4-e116e8b3.pth',\n",
    "    'efficientnet-b5': 'http://storage.googleapis.com/public-models/efficientnet-b5-586e6cc6.pth',\n",
    "}\n",
    "\n",
    "def load_pretrained_weights(model, model_name, load_fc=True):\n",
    "    \"\"\" Loads pretrained weights, and downloads if loading for the first time. \"\"\"\n",
    "    state_dict = model_zoo.load_url(url_map[model_name])\n",
    "    if load_fc:\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        state_dict.pop('_fc.weight')\n",
    "        state_dict.pop('_fc.bias')\n",
    "        res = model.load_state_dict(state_dict, strict=False)\n",
    "        assert str(res.missing_keys) == str(['_fc.weight', '_fc.bias']), 'issue loading pretrained weights'\n",
    "    print('Loaded pretrained weights for {}'.format(model_name))\n",
    "    \n",
    "    \n",
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Mobile Inverted Residual Bottleneck Block\n",
    "\n",
    "    Args:\n",
    "        block_args (namedtuple): BlockArgs, see above\n",
    "        global_params (namedtuple): GlobalParam, see above\n",
    "\n",
    "    Attributes:\n",
    "        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_args, global_params):\n",
    "        super().__init__()\n",
    "        self._block_args = block_args\n",
    "        self._bn_mom = 1 - global_params.batch_norm_momentum\n",
    "        self._bn_eps = global_params.batch_norm_epsilon\n",
    "        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n",
    "        self.id_skip = block_args.id_skip  # skip connection and drop connect\n",
    "\n",
    "        # Get static or dynamic convolution depending on image size\n",
    "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
    "\n",
    "        # Expansion phase\n",
    "        inp = self._block_args.input_filters  # number of input channels\n",
    "        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n",
    "        if self._block_args.expand_ratio != 1:\n",
    "            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
    "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        k = self._block_args.kernel_size\n",
    "        s = self._block_args.stride\n",
    "        self._depthwise_conv = Conv2d(\n",
    "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
    "            kernel_size=k, stride=s, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Squeeze and Excitation layer, if desired\n",
    "        if self.has_se:\n",
    "            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n",
    "            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
    "            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
    "\n",
    "        # Output phase\n",
    "        final_oup = self._block_args.output_filters\n",
    "        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
    "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "    def forward(self, inputs, drop_connect_rate=None):\n",
    "        \"\"\"\n",
    "        :param inputs: input tensor\n",
    "        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n",
    "        :return: output of block\n",
    "        \"\"\"\n",
    "\n",
    "        # Expansion and Depthwise Convolution\n",
    "        x = inputs\n",
    "        if self._block_args.expand_ratio != 1:\n",
    "            x = relu_fn(self._bn0(self._expand_conv(inputs)))\n",
    "        x = relu_fn(self._bn1(self._depthwise_conv(x)))\n",
    "\n",
    "        # Squeeze and Excitation\n",
    "        if self.has_se:\n",
    "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "            x_squeezed = self._se_expand(relu_fn(self._se_reduce(x_squeezed)))\n",
    "            x = torch.sigmoid(x_squeezed) * x\n",
    "\n",
    "        x = self._bn2(self._project_conv(x))\n",
    "\n",
    "        # Skip connection and drop connect\n",
    "        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n",
    "        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n",
    "            if drop_connect_rate:\n",
    "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
    "            x = x + inputs  # skip connection\n",
    "        return x\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    \"\"\"\n",
    "    An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n",
    "\n",
    "    Args:\n",
    "        blocks_args (list): A list of BlockArgs to construct blocks\n",
    "        global_params (namedtuple): A set of GlobalParams shared between blocks\n",
    "\n",
    "    Example:\n",
    "        model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, blocks_args=None, global_params=None):\n",
    "        super().__init__()\n",
    "        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n",
    "        assert len(blocks_args) > 0, 'block args must be greater than 0'\n",
    "        self._global_params = global_params\n",
    "        self._blocks_args = blocks_args\n",
    "\n",
    "        # Get static or dynamic convolution depending on image size\n",
    "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
    "\n",
    "        # Batch norm parameters\n",
    "        bn_mom = 1 - self._global_params.batch_norm_momentum\n",
    "        bn_eps = self._global_params.batch_norm_epsilon\n",
    "\n",
    "        # Stem\n",
    "        in_channels = 3  # rgb\n",
    "        out_channels = round_filters(32, self._global_params)  # number of output channels\n",
    "        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
    "        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Build blocks\n",
    "        self._blocks = nn.ModuleList([])\n",
    "        for block_args in self._blocks_args:\n",
    "\n",
    "            # Update block input and output filters based on depth multiplier.\n",
    "            block_args = block_args._replace(\n",
    "                input_filters=round_filters(block_args.input_filters, self._global_params),\n",
    "                output_filters=round_filters(block_args.output_filters, self._global_params),\n",
    "                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n",
    "            )\n",
    "\n",
    "            # The first block needs to take care of stride and filter size increase.\n",
    "            self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
    "            if block_args.num_repeat > 1:\n",
    "                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n",
    "            for _ in range(block_args.num_repeat - 1):\n",
    "                self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
    "\n",
    "        # Head\n",
    "        in_channels = block_args.output_filters  # output of final block\n",
    "        out_channels = round_filters(1280, self._global_params)\n",
    "        self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Final linear layer\n",
    "        self._dropout = self._global_params.dropout_rate\n",
    "        self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n",
    "\n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
    "\n",
    "        # Stem\n",
    "        x = relu_fn(self._bn0(self._conv_stem(inputs)))\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self._blocks):\n",
    "            drop_connect_rate = self._global_params.drop_connect_rate\n",
    "            if drop_connect_rate:\n",
    "                drop_connect_rate *= float(idx) / len(self._blocks)\n",
    "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "\n",
    "        # Head\n",
    "        x = relu_fn(self._bn1(self._conv_head(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n",
    "\n",
    "        # Convolution layers\n",
    "        x = self.extract_features(inputs)\n",
    "\n",
    "        # Pooling and final linear layer\n",
    "        x = F.adaptive_avg_pool2d(x, 1).squeeze(-1).squeeze(-1)\n",
    "        if self._dropout:\n",
    "            x = F.dropout(x, p=self._dropout, training=self.training)\n",
    "        x = self._fc(x)\n",
    "        return x\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, model_name, override_params=None):\n",
    "        cls._check_model_name_is_valid(model_name)\n",
    "        blocks_args, global_params = get_model_params(model_name, override_params)\n",
    "        return EfficientNet(blocks_args, global_params)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name, num_classes=1000):\n",
    "        model = EfficientNet.from_name(model_name, override_params={'num_classes': num_classes})\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def get_image_size(cls, model_name):\n",
    "        cls._check_model_name_is_valid(model_name)\n",
    "        _, _, res, _ = efficientnet_params(model_name)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _check_model_name_is_valid(cls, model_name, also_need_pretrained_weights=False):\n",
    "        \"\"\" Validates model name. None that pretrained weights are only available for\n",
    "        the first four models (efficientnet-b{i} for i in 0,1,2,3) at the moment. \"\"\"\n",
    "        num_models = 4 if also_need_pretrained_weights else 8\n",
    "        valid_models = ['efficientnet_b'+str(i) for i in range(num_models)]\n",
    "        if model_name.replace('-','_') not in valid_models:\n",
    "            raise ValueError('model_name should be one of: ' + ', '.join(valid_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.core import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.torch_core import *\n",
    "import scipy as sp\n",
    "from functools import partial\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def _tta_only(learn:Learner, ds_type:DatasetType=DatasetType.Valid, num_pred:int=10) -> Iterator[List[Tensor]]:\n",
    "    \"Computes the outputs for several augmented inputs for TTA\"\n",
    "    dl = learn.dl(ds_type)\n",
    "    ds = dl.dataset\n",
    "    old = ds.tfms\n",
    "#     aug_tfms = [o for o in learn.data.train_ds.tfms]\n",
    "    aug_tfms = [RandTransform(tfm=TfmAffine (dihedral_affine), kwargs={}, p=1.0, resolved={}, do_run=True, is_random=True, use_on_y=True), \n",
    "                RandTransform(tfm=TfmAffine (rotate), kwargs={'degrees': (-360, 360)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True)]\n",
    "    \n",
    "    try:\n",
    "        pbar = master_bar(range(num_pred))\n",
    "        for i in pbar:\n",
    "            ds.tfms = aug_tfms\n",
    "            yield get_preds(learn.model, dl, pbar=pbar)[0]\n",
    "    finally: ds.tfms = old\n",
    "    \n",
    "Learner.tta_only = _tta_only\n",
    "\n",
    "\n",
    "def _TTA(learn:Learner, beta:float=0, ds_type:DatasetType=DatasetType.Valid, num_pred:int=10, with_loss:bool=False) -> Tensors:\n",
    "    \"Applies TTA to predict on `ds_type` dataset.\"\n",
    "    preds,y = learn.get_preds(ds_type)\n",
    "    all_preds = list(learn.tta_only(ds_type=ds_type, num_pred=num_pred))\n",
    "    avg_preds = torch.stack(all_preds).mean(0)\n",
    "    if beta is None: return preds,avg_preds,y\n",
    "    else:            \n",
    "        final_preds = preds*beta + avg_preds*(1-beta)\n",
    "        if with_loss: \n",
    "            with NoneReduceOnCPU(learn.loss_func) as lf: loss = lf(final_preds, y)\n",
    "            return final_preds, y, loss\n",
    "        return final_preds, y\n",
    "\n",
    "Learner.TTA = _TTA\n",
    "\n",
    "\n",
    "__all__ = ['brightness', 'contrast', 'crop', 'crop_pad', 'cutout', 'dihedral', 'dihedral_affine', 'flip_affine', 'flip_lr',\n",
    "           'get_transforms', 'jitter', 'pad', 'perspective_warp', 'rand_pad', 'rand_crop', 'rand_zoom', 'rgb_randomize', 'rotate', 'skew', 'squish',\n",
    "           'rand_resize_crop', 'symmetric_warp', 'tilt', 'zoom', 'zoom_crop']\n",
    "\n",
    "_pad_mode_convert = {'reflection':'reflect', 'zeros':'constant', 'border':'replicate'}\n",
    "\n",
    "#NB: Although TfmLighting etc can be used as decorators, that doesn't work in Windows,\n",
    "#    so we do it manually for now.\n",
    "\n",
    "def _brightness(x, change:uniform):\n",
    "    \"Apply `change` in brightness of image `x`.\"\n",
    "    return x.add_(scipy.special.logit(change))\n",
    "brightness = TfmLighting(_brightness)\n",
    "\n",
    "def _contrast(x, scale:log_uniform):\n",
    "    \"Apply `scale` to contrast of image `x`.\"\n",
    "    return x.mul_(scale)\n",
    "contrast = TfmLighting(_contrast)\n",
    "\n",
    "def _rotate(degrees:uniform):\n",
    "    \"Rotate image by `degrees`.\"\n",
    "    angle = degrees * math.pi / 180\n",
    "    return [[cos(angle), -sin(angle), 0.],\n",
    "            [sin(angle),  cos(angle), 0.],\n",
    "            [0.        ,  0.        , 1.]]\n",
    "rotate = TfmAffine(_rotate)\n",
    "\n",
    "def _get_zoom_mat(sw:float, sh:float, c:float, r:float)->AffineMatrix:\n",
    "    \"`sw`,`sh` scale width,height - `c`,`r` focus col,row.\"\n",
    "    return [[sw, 0,  c],\n",
    "            [0, sh,  r],\n",
    "            [0,  0, 1.]]\n",
    "\n",
    "def _zoom(scale:uniform=1.0, row_pct:uniform=0.5, col_pct:uniform=0.5):\n",
    "    \"Zoom image by `scale`. `row_pct`,`col_pct` select focal point of zoom.\"\n",
    "    s = 1-1/scale\n",
    "    col_c = s * (2*col_pct - 1)\n",
    "    row_c = s * (2*row_pct - 1)\n",
    "    return _get_zoom_mat(1/scale, 1/scale, col_c, row_c)\n",
    "zoom = TfmAffine(_zoom)\n",
    "\n",
    "def _squish(scale:uniform=1.0, row_pct:uniform=0.5, col_pct:uniform=0.5):\n",
    "    \"Squish image by `scale`. `row_pct`,`col_pct` select focal point of zoom.\"\n",
    "    if scale <= 1:\n",
    "        col_c = (1-scale) * (2*col_pct - 1)\n",
    "        return _get_zoom_mat(scale, 1, col_c, 0.)\n",
    "    else:\n",
    "        row_c = (1-1/scale) * (2*row_pct - 1)\n",
    "        return _get_zoom_mat(1, 1/scale, 0., row_c)\n",
    "squish = TfmAffine(_squish)\n",
    "\n",
    "def _jitter(c, magnitude:uniform):\n",
    "    \"Replace pixels by random neighbors at `magnitude`.\"\n",
    "    c.flow.add_((torch.rand_like(c.flow)-0.5)*magnitude*2)\n",
    "    return c\n",
    "jitter = TfmCoord(_jitter)\n",
    "\n",
    "def _flip_lr(x):\n",
    "    \"Flip `x` horizontally.\"\n",
    "    #return x.flip(2)\n",
    "    if isinstance(x, ImagePoints):\n",
    "        x.flow.flow[...,0] *= -1\n",
    "        return x\n",
    "    return tensor(np.ascontiguousarray(np.array(x)[...,::-1]))\n",
    "flip_lr = TfmPixel(_flip_lr)\n",
    "\n",
    "def _flip_affine() -> TfmAffine:\n",
    "    \"Flip `x` horizontally.\"\n",
    "    return [[-1, 0, 0.],\n",
    "            [0,  1, 0],\n",
    "            [0,  0, 1.]]\n",
    "flip_affine = TfmAffine(_flip_affine)\n",
    "\n",
    "def _dihedral(x, k:partial(uniform_int,0,7)):\n",
    "    \"Randomly flip `x` image based on `k`.\"\n",
    "    flips=[]\n",
    "    if k&1: flips.append(1)\n",
    "    if k&2: flips.append(2)\n",
    "    if flips: x = torch.flip(x,flips)\n",
    "    if k&4: x = x.transpose(1,2)\n",
    "    return x.contiguous()\n",
    "dihedral = TfmPixel(_dihedral)\n",
    "\n",
    "def _dihedral_affine(k:partial(uniform_int,0,7)):\n",
    "    \"Randomly flip `x` image based on `k`.\"\n",
    "    x = -1 if k&1 else 1\n",
    "    y = -1 if k&2 else 1\n",
    "    if k&4: return [[0, x, 0.],\n",
    "                    [y, 0, 0],\n",
    "                    [0, 0, 1.]]\n",
    "    return [[x, 0, 0.],\n",
    "            [0, y, 0],\n",
    "            [0, 0, 1.]]\n",
    "dihedral_affine = TfmAffine(_dihedral_affine)\n",
    "\n",
    "def _pad_coord(x, row_pad:int, col_pad:int, mode='zeros'):\n",
    "    #TODO: implement other padding modes than zeros?\n",
    "    h,w = x.size\n",
    "    pad = torch.Tensor([w/(w + 2*col_pad), h/(h + 2*row_pad)])\n",
    "    x.flow = FlowField((h+2*row_pad, w+2*col_pad) , x.flow.flow * pad[None])\n",
    "    return x\n",
    "\n",
    "def _pad_default(x, padding:int, mode='reflection'):\n",
    "    \"Pad `x` with `padding` pixels. `mode` fills in space ('zeros','reflection','border').\"\n",
    "    mode = _pad_mode_convert[mode]\n",
    "    return F.pad(x[None], (padding,)*4, mode=mode)[0]\n",
    "\n",
    "def _pad_image_points(x, padding:int, mode='reflection'):\n",
    "    return _pad_coord(x, padding, padding, mode)\n",
    "\n",
    "def _pad(x, padding:int, mode='reflection'):\n",
    "    f_pad = _pad_image_points if isinstance(x, ImagePoints) else  _pad_default\n",
    "    return f_pad(x, padding, mode)\n",
    "\n",
    "pad = TfmPixel(_pad, order=-10)\n",
    "\n",
    "def _cutout(x, n_holes:uniform_int=1, length:uniform_int=40):\n",
    "    \"Cut out `n_holes` number of square holes of size `length` in image at random locations.\"\n",
    "    h,w = x.shape[1:]\n",
    "    for n in range(n_holes):\n",
    "        h_y = np.random.randint(0, h)\n",
    "        h_x = np.random.randint(0, w)\n",
    "        y1 = int(np.clip(h_y - length / 2, 0, h))\n",
    "        y2 = int(np.clip(h_y + length / 2, 0, h))\n",
    "        x1 = int(np.clip(h_x - length / 2, 0, w))\n",
    "        x2 = int(np.clip(h_x + length / 2, 0, w))\n",
    "        x[:, y1:y2, x1:x2] = 0\n",
    "    return x\n",
    "\n",
    "cutout = TfmPixel(_cutout, order=20)\n",
    "\n",
    "def _rgb_randomize(x, channel:int=None, thresh:float=0.3):\n",
    "    \"Randomize one of the channels of the input image\"\n",
    "    if channel is None: channel = np.random.randint(0, x.shape[0] - 1)\n",
    "    x[channel] = torch.rand(x.shape[1:]) * np.random.uniform(0, thresh)\n",
    "    return x\n",
    "\n",
    "rgb_randomize = TfmPixel(_rgb_randomize)\n",
    "\n",
    "def _minus_epsilon(row_pct:float, col_pct:float, eps:float=1e-7):\n",
    "    if row_pct==1.: row_pct -= 1e-7\n",
    "    if col_pct==1.: col_pct -= 1e-7\n",
    "    return row_pct,col_pct\n",
    "\n",
    "def _crop_default(x, size, row_pct:uniform=0.5, col_pct:uniform=0.5):\n",
    "    \"Crop `x` to `size` pixels. `row_pct`,`col_pct` select focal point of crop.\"\n",
    "    rows,cols = tis2hw(size)\n",
    "    row_pct,col_pct = _minus_epsilon(row_pct,col_pct)\n",
    "    row = int((x.size(1)-rows+1) * row_pct)\n",
    "    col = int((x.size(2)-cols+1) * col_pct)\n",
    "    return x[:, row:row+rows, col:col+cols].contiguous()\n",
    "\n",
    "def _crop_image_points(x, size, row_pct=0.5, col_pct=0.5):\n",
    "    h,w = x.size\n",
    "    rows,cols = tis2hw(size)\n",
    "    row_pct,col_pct = _minus_epsilon(row_pct,col_pct)\n",
    "    x.flow.flow.mul_(torch.Tensor([w/cols, h/rows])[None])\n",
    "    row = int((h-rows+1) * row_pct)\n",
    "    col = int((w-cols+1) * col_pct)\n",
    "    x.flow.flow.add_(-1 + torch.Tensor([w/cols-2*col/cols, h/rows-2*row/rows])[None])\n",
    "    x.size = (rows, cols)\n",
    "    return x\n",
    "\n",
    "def _crop(x, size, row_pct:uniform=0.5, col_pct:uniform=0.5):\n",
    "    f_crop = _crop_image_points if isinstance(x, ImagePoints) else _crop_default\n",
    "    return f_crop(x, size, row_pct, col_pct)\n",
    "\n",
    "crop = TfmPixel(_crop)\n",
    "\n",
    "def _crop_pad_default(x, size, padding_mode='reflection', row_pct:uniform = 0.5, col_pct:uniform = 0.5):\n",
    "    \"Crop and pad tfm - `row_pct`,`col_pct` sets focal point.\"\n",
    "    padding_mode = _pad_mode_convert[padding_mode]\n",
    "    size = tis2hw(size)\n",
    "    if x.shape[1:] == torch.Size(size): return x\n",
    "    rows,cols = size\n",
    "    row_pct,col_pct = _minus_epsilon(row_pct,col_pct)\n",
    "    if x.size(1)<rows or x.size(2)<cols:\n",
    "        row_pad = max((rows-x.size(1)+1)//2, 0)\n",
    "        col_pad = max((cols-x.size(2)+1)//2, 0)\n",
    "        x = F.pad(x[None], (col_pad,col_pad,row_pad,row_pad), mode=padding_mode)[0]\n",
    "    row = int((x.size(1)-rows+1)*row_pct)\n",
    "    col = int((x.size(2)-cols+1)*col_pct)\n",
    "    x = x[:, row:row+rows, col:col+cols]\n",
    "    return x.contiguous() # without this, get NaN later - don't know why\n",
    "\n",
    "def _crop_pad_image_points(x, size, padding_mode='reflection', row_pct = 0.5, col_pct = 0.5):\n",
    "    size = tis2hw(size)\n",
    "    rows,cols = size\n",
    "    if x.size[0]<rows or x.size[1]<cols:\n",
    "        row_pad = max((rows-x.size[0]+1)//2, 0)\n",
    "        col_pad = max((cols-x.size[1]+1)//2, 0)\n",
    "        x = _pad_coord(x, row_pad, col_pad)\n",
    "    return crop(x,(rows,cols), row_pct, col_pct)\n",
    "\n",
    "def _crop_pad(x, size, padding_mode='reflection', row_pct:uniform = 0.5, col_pct:uniform = 0.5):\n",
    "    f_crop_pad = _crop_pad_image_points if isinstance(x, ImagePoints) else _crop_pad_default\n",
    "    return f_crop_pad(x, size, padding_mode, row_pct, col_pct)\n",
    "\n",
    "crop_pad = TfmCrop(_crop_pad)\n",
    "\n",
    "def _image_maybe_add_crop_pad(img, tfms):\n",
    "    tfm_names = [tfm.__name__ for tfm in tfms]\n",
    "    return [crop_pad()] + tfms if 'crop_pad' not in tfm_names else tfms\n",
    "Image._maybe_add_crop_pad = _image_maybe_add_crop_pad\n",
    "\n",
    "rand_pos = {'row_pct':(0,1), 'col_pct':(0,1)}\n",
    "\n",
    "def rand_pad(padding:int, size:int, mode:str='reflection'):\n",
    "    \"Fixed `mode` `padding` and random crop of `size`\"\n",
    "    return [pad(padding=padding,mode=mode),\n",
    "            crop(size=size, **rand_pos)]\n",
    "\n",
    "def rand_zoom(scale:uniform=1.0, p:float=1.):\n",
    "    \"Randomized version of `zoom`.\"\n",
    "    return zoom(scale=scale, **rand_pos, p=p)\n",
    "\n",
    "def rand_crop(*args, padding_mode='reflection', p:float=1.):\n",
    "    \"Randomized version of `crop_pad`.\"\n",
    "    return crop_pad(*args, **rand_pos, padding_mode=padding_mode, p=p)\n",
    "\n",
    "def zoom_crop(scale:float, do_rand:bool=False, p:float=1.0):\n",
    "    \"Randomly zoom and/or crop.\"\n",
    "    zoom_fn = rand_zoom if do_rand else zoom\n",
    "    crop_fn = rand_crop if do_rand else crop_pad\n",
    "    return [zoom_fn(scale=scale, p=p), crop_fn()]\n",
    "\n",
    "_solve_func = getattr(torch, 'solve', None)\n",
    "if _solve_func is None: _solve_func = torch.gesv\n",
    "\n",
    "def _find_coeffs(orig_pts:Points, targ_pts:Points)->Tensor:\n",
    "    \"Find 8 coeff mentioned [here](https://web.archive.org/web/20150222120106/xenia.media.mit.edu/~cwren/interpolator/).\"\n",
    "    matrix = []\n",
    "    #The equations we'll need to solve.\n",
    "    for p1, p2 in zip(targ_pts, orig_pts):\n",
    "        matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1]])\n",
    "        matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1]*p1[0], -p2[1]*p1[1]])\n",
    "\n",
    "    A = FloatTensor(matrix)\n",
    "    B = FloatTensor(orig_pts).view(8, 1)\n",
    "    #The 8 scalars we seek are solution of AX = B\n",
    "    return _solve_func(B,A)[0][:,0]\n",
    "\n",
    "def _apply_perspective(coords:FlowField, coeffs:Points)->FlowField:\n",
    "    \"Transform `coords` with `coeffs`.\"\n",
    "    size = coords.flow.size()\n",
    "    #compress all the dims expect the last one ang adds ones, coords become N * 3\n",
    "    coords.flow = coords.flow.view(-1,2)\n",
    "    #Transform the coeffs in a 3*3 matrix with a 1 at the bottom left\n",
    "    coeffs = torch.cat([coeffs, FloatTensor([1])]).view(3,3)\n",
    "    coords.flow = torch.addmm(coeffs[:,2], coords.flow, coeffs[:,:2].t())\n",
    "    coords.flow.mul_(1/coords.flow[:,2].unsqueeze(1))\n",
    "    coords.flow = coords.flow[:,:2].view(size)\n",
    "    return coords\n",
    "\n",
    "_orig_pts = [[-1,-1], [-1,1], [1,-1], [1,1]]\n",
    "\n",
    "def _do_perspective_warp(c:FlowField, targ_pts:Points, invert=False):\n",
    "    \"Apply warp to `targ_pts` from `_orig_pts` to `c` `FlowField`.\"\n",
    "    if invert: return _apply_perspective(c, _find_coeffs(targ_pts, _orig_pts))\n",
    "    return _apply_perspective(c, _find_coeffs(_orig_pts, targ_pts))\n",
    "\n",
    "def _perspective_warp(c, magnitude:partial(uniform,size=8)=0, invert=False):\n",
    "    \"Apply warp of `magnitude` to `c`.\"\n",
    "    magnitude = magnitude.view(4,2)\n",
    "    targ_pts = [[x+m for x,m in zip(xs, ms)] for xs, ms in zip(_orig_pts, magnitude)]\n",
    "    return _do_perspective_warp(c, targ_pts, invert)\n",
    "perspective_warp = TfmCoord(_perspective_warp)\n",
    "\n",
    "def _symmetric_warp(c, magnitude:partial(uniform,size=4)=0, invert=False):\n",
    "    \"Apply symmetric warp of `magnitude` to `c`.\"\n",
    "    m = listify(magnitude, 4)\n",
    "    targ_pts = [[-1-m[3],-1-m[1]], [-1-m[2],1+m[1]], [1+m[3],-1-m[0]], [1+m[2],1+m[0]]]\n",
    "    return _do_perspective_warp(c, targ_pts, invert)\n",
    "symmetric_warp = TfmCoord(_symmetric_warp)\n",
    "\n",
    "def _tilt(c, direction:uniform_int, magnitude:uniform=0, invert=False):\n",
    "    \"Tilt `c` field with random `direction` and `magnitude`.\"\n",
    "    orig_pts = [[-1,-1], [-1,1], [1,-1], [1,1]]\n",
    "    if direction == 0:   targ_pts = [[-1,-1], [-1,1], [1,-1-magnitude], [1,1+magnitude]]\n",
    "    elif direction == 1: targ_pts = [[-1,-1-magnitude], [-1,1+magnitude], [1,-1], [1,1]]\n",
    "    elif direction == 2: targ_pts = [[-1,-1], [-1-magnitude,1], [1,-1], [1+magnitude,1]]\n",
    "    elif direction == 3: targ_pts = [[-1-magnitude,-1], [-1,1], [1+magnitude,-1], [1,1]]\n",
    "    coeffs = _find_coeffs(targ_pts, _orig_pts) if invert else _find_coeffs(_orig_pts, targ_pts)\n",
    "    return _apply_perspective(c, coeffs)\n",
    "tilt = TfmCoord(_tilt)\n",
    "\n",
    "def _skew(c, direction:uniform_int, magnitude:uniform=0, invert=False):\n",
    "    \"Skew `c` field with random `direction` and `magnitude`.\"\n",
    "    orig_pts = [[-1,-1], [-1,1], [1,-1], [1,1]]\n",
    "    if direction == 0:   targ_pts = [[-1-magnitude,-1], [-1,1], [1,-1], [1,1]]\n",
    "    elif direction == 1: targ_pts = [[-1,-1-magnitude], [-1,1], [1,-1], [1,1]]\n",
    "    elif direction == 2: targ_pts = [[-1,-1], [-1-magnitude,1], [1,-1], [1,1]]\n",
    "    elif direction == 3: targ_pts = [[-1,-1], [-1,1+magnitude], [1,-1], [1,1]]\n",
    "    elif direction == 4: targ_pts = [[-1,-1], [-1,1], [1+magnitude,-1], [1,1]]\n",
    "    elif direction == 5: targ_pts = [[-1,-1], [-1,1], [1,-1-magnitude], [1,1]]\n",
    "    elif direction == 6: targ_pts = [[-1,-1], [-1,1], [1,-1], [1+magnitude,1]]\n",
    "    elif direction == 7: targ_pts = [[-1,-1], [-1,1], [1,-1], [1,1+magnitude]]\n",
    "    coeffs = _find_coeffs(targ_pts, _orig_pts) if invert else _find_coeffs(_orig_pts, targ_pts)\n",
    "    return _apply_perspective(c, coeffs)\n",
    "skew = TfmCoord(_skew)\n",
    "\n",
    "\n",
    "def get_transforms(do_flip:bool=True, flip_vert:bool=False, max_rotate:float=10., max_zoom:float=1.1,\n",
    "                   max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75,\n",
    "                   p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None)->Collection[Transform]:\n",
    "    \"Utility func to easily create a list of flip, rotate, `zoom`, warp, lighting transforms.\"\n",
    "    res = []\n",
    "    if do_flip:    res.append(dihedral_affine() if flip_vert else flip_lr(p=0.5))\n",
    "    if max_warp:   res.append(symmetric_warp(magnitude=(-max_warp,max_warp), p=p_affine))\n",
    "    if max_rotate: res.append(rotate(degrees=(-max_rotate,max_rotate), p=p_affine))\n",
    "    if max_zoom>1: res.append(zoom(scale=(1.,max_zoom),row_pct=0.5,col_pct=0.5, p=p_affine))\n",
    "    if max_lighting:\n",
    "        res.append(brightness(change=(0.5*(1-max_lighting), 0.5*(1+max_lighting)), p=p_lighting))\n",
    "        res.append(contrast(scale=(1-max_lighting, 1/(1-max_lighting)), p=p_lighting))\n",
    "    #       train                   , valid\n",
    "    return (res + listify(xtra_tfms), [crop_pad()])\n",
    "\n",
    "def _compute_zs_mat(sz:TensorImageSize, scale:float, squish:float,\n",
    "                   invert:bool, row_pct:float, col_pct:float)->AffineMatrix:\n",
    "    \"Utility routine to compute zoom/squish matrix.\"\n",
    "    orig_ratio = math.sqrt(sz[1]/sz[0])\n",
    "    for s,r,i in zip(scale,squish, invert):\n",
    "        s,r = 1/math.sqrt(s),math.sqrt(r)\n",
    "        if s * r <= 1 and s / r <= 1: #Test if we are completely inside the picture\n",
    "            w,h = (s/r, s*r) if i else (s*r,s/r)\n",
    "            col_c = (1-w) * (2*col_pct - 1)\n",
    "            row_c = (1-h) * (2*row_pct - 1)\n",
    "            return _get_zoom_mat(w, h, col_c, row_c)\n",
    "\n",
    "    #Fallback, hack to emulate a center crop without cropping anything yet.\n",
    "    if orig_ratio > 1: return _get_zoom_mat(1/orig_ratio**2, 1, 0, 0.)\n",
    "    else:              return _get_zoom_mat(1, orig_ratio**2, 0, 0.)\n",
    "\n",
    "def _zoom_squish(c, scale:uniform=1.0, squish:uniform=1.0, invert:rand_bool=False,\n",
    "                row_pct:uniform=0.5, col_pct:uniform=0.5):\n",
    "    #This is intended for scale, squish and invert to be of size 10 (or whatever) so that the transform\n",
    "    #can try a few zoom/squishes before falling back to center crop (like torchvision.RandomResizedCrop)\n",
    "    m = _compute_zs_mat(c.size, scale, squish, invert, row_pct, col_pct)\n",
    "    return _affine_mult(c, FloatTensor(m))\n",
    "zoom_squish = TfmCoord(_zoom_squish)\n",
    "\n",
    "def rand_resize_crop(size:int, max_scale:float=2., ratios:Tuple[float,float]=(0.75,1.33)):\n",
    "    \"Randomly resize and crop the image to a ratio in `ratios` after a zoom of `max_scale`.\"\n",
    "    return [zoom_squish(scale=(1.,max_scale,8), squish=(*ratios,8), invert=(0.5,8), row_pct=(0.,1.), col_pct=(0.,1.)),\n",
    "            crop(size=size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3662, 2)\n",
      "(35126, 2)\n"
     ]
    }
   ],
   "source": [
    "train2019 = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
    "train2015 = pd.read_csv('../input/diabetic-retinopathy-resized/trainLabels.csv')\n",
    "print(train2019.shape)\n",
    "print(train2015.shape)\n",
    "\n",
    "train2015 = train2015[['image','level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2015.columns = train2019.columns\n",
    "train2015.diagnosis.value_counts()\n",
    "\n",
    "# path columns\n",
    "train2019['id_code'] = '../input/aptos2019-blindness-detection/train_images/' + train2019['id_code'].astype(str) + '.png'\n",
    "train2015['id_code'] = '../input/diabetic-retinopathy-resized/resized_train/resized_train/' + train2015['id_code'].astype(str) + '.jpeg'\n",
    "train_dataset = pd.concat([train2015, train2019], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qk(y_pred, y):\n",
    "    return torch.tensor(cohen_kappa_score(torch.round(y_pred), y, weights='quadratic'), device='cuda:0')\n",
    "\n",
    "\n",
    "def get_df():\n",
    "    base_image_dir = os.path.join('..', 'input/aptos2019-blindness-detection/')\n",
    "    train_dir = os.path.join(base_image_dir,'train_images/')\n",
    "    df = pd.read_csv(os.path.join(base_image_dir, 'train.csv'))\n",
    "    df['path'] = df['id_code'].map(lambda x: os.path.join(train_dir,'{}.png'.format(x)))\n",
    "    df = df.drop(columns=['id_code'])\n",
    "    df = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\n",
    "    test_df = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "    return df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models\n",
    "!cp '../input/kaggle-public/abcdef.pth' 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making model\n",
    "md_ef = EfficientNet.from_pretrained('efficientnet-b5', num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can play around with tfms and image sizes\n",
    "df, test_df = get_df()\n",
    "\n",
    "bs = 32\n",
    "sz = 328\n",
    "\n",
    "tfms = get_transforms(do_flip=True, flip_vert=True, max_rotate=180, max_zoom=1.0, \n",
    "                      max_lighting=0.2, max_warp=0, p_affine=0.75, p_lighting=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2' class='' max='3', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      66.67% [2/3 1:22:19<41:09]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>qk</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.488599</td>\n",
       "      <td>0.479647</td>\n",
       "      <td>0.605820</td>\n",
       "      <td>40:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.379098</td>\n",
       "      <td>0.384277</td>\n",
       "      <td>0.711045</td>\n",
       "      <td>41:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='56' class='' max='243', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      23.05% [56/243 01:14<04:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.47964659333229065.\n",
      "Better model found at epoch 1 with valid_loss value: 0.38427653908729553.\n"
     ]
    }
   ],
   "source": [
    " data = (ImageList.from_df(df=train_dataset,path='./',cols='id_code') \n",
    "         .split_by_rand_pct(0.2) \n",
    "         .label_from_df(cols='diagnosis',label_cls=FloatList) \n",
    "         .transform(tfms,size=sz,resize_method=ResizeMethod.SQUISH,padding_mode='zeros') \n",
    "         .databunch(bs=bs,num_workers=4) \n",
    "         .normalize(imagenet_stats)  \n",
    "        )\n",
    "\n",
    "learn = Learner(data, \n",
    "                md_ef, \n",
    "                metrics = [qk], \n",
    "                model_dir=\"models\").to_fp16()\n",
    "\n",
    "learn.load('pretrain_model')",
    "learn.fit_one_cycle(10, max_lr=1e-3,\n",
    "                    callbacks=[SaveModelCallback(learn, every='improvement', monitor='valid_loss', mode='min', name=name_list[i+1]+'_best')])\n",
    "learn.save('final_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.add_test(ImageList.from_df(test_df,'../input/aptos2019-blindness-detection',folder='test_images',suffix='.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
